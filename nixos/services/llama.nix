{ lib, config, ... }:
# wait for 24.05
{
  # services.llama-cpp = {
  #   enable = true;
  #   port = 9920;
  #   openFirewall = true;
  #   model = "/data/llama/ggml-model-q4_0.gguf";
  # };
  #
  # systemd.tmpfiles.rules = [
  #   "d /data/llama/ 0777 nixos users"
  # ];
}
